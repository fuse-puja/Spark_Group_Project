{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/06 10:36:26 WARN Utils: Your hostname, LAPTOP-GRLQJBNM resolves to a loopback address: 127.0.1.1; using 172.27.87.196 instead (on interface eth0)\n",
      "23/09/06 10:36:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/06 10:36:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining path to the dataset\n",
    "customer_data_path = \"./Data/olist_customers_dataset.csv\"  # Replace with the actual path\n",
    "order_item_path = \"./Data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"./Data/olist_order_payments_dataset.csv\"\n",
    "product_category_translation_path= \"./Data/product_category_name_translation.csv\"\n",
    "product_path = './Data/olist_products_dataset.csv'\n",
    "seller_path = './Data/olist_sellers_dataset.csv'\n",
    "geolocation_path = './Data/olist_geolocation_dataset.csv'\n",
    "orders_path = './Data/olist_orders_dataset.csv'\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "customer_df = spark.read.csv(customer_data_path, header=True, inferSchema=True)\n",
    "order_item_df = spark.read.csv(order_item_path, header=True, inferSchema=True)\n",
    "order_payment_df = spark.read.csv(order_payment_path, header=True, inferSchema=True)\n",
    "product_category_translation_df = spark.read.csv(product_category_translation_path, header=True, inferSchema=True)\n",
    "seller_df_uncleaned = spark.read.csv(seller_path, header=True, inferSchema=True)\n",
    "product_df_uncleaned = spark.read.csv(product_path, header=True, inferSchema=True)\n",
    "geoloacation_df_uncleaned = spark.read.csv(geolocation_path, header=True, inferSchema= True)\n",
    "orders_df_uncleaned = spark.read.csv(orders_path, header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim,regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "seller_df_uncleaned.select([trim(col(c)).alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "# Remove whitespace characters between words in all columns\n",
    "seller_df = seller_df_uncleaned.select([regexp_replace(col(c), r'\\s+', ' ').alias(c) for c in seller_df_uncleaned.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       são paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       são paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       são paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "geoloacation_df_uncleaned.select([trim(col(c)).alias(c) for c in geoloacation_df_uncleaned.columns])\n",
    "\n",
    "geoloacation_df_uncleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       sao paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       sao paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace \"são paulo\" with \"sao paulo\" in the geolocation dataframe\n",
    "geolocation_df = geoloacation_df_uncleaned.replace(\"são paulo\", \"sao paulo\")\n",
    "\n",
    "# Show the DataFrame with the replaced values\n",
    "geolocation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows in uncleaned dataset =  99441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows of cleaned datset =  96461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the 'orders_df_uncleaned' DataFrame\n",
    "print(\"No of rows in uncleaned dataset = \", orders_df_uncleaned.count())\n",
    "\n",
    "# Drop rows with null values in the 'orders_df_uncleaned' DataFrame\n",
    "orders_df = orders_df_uncleaned.dropna()\n",
    "\n",
    "# Print the number of rows in the 'orders_df' DataFrame after dropping null values\n",
    "print(\"No of rows of cleaned datset = \", orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing column on product dataset with content from product category translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|          product_id|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_category_name|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|1e9e8ef04dbcff454...|                 40|                       287|                 1|             225|               16|               10|              14|            perfumery|\n",
      "|3aa071139cb16b67c...|                 44|                       276|                 1|            1000|               30|               18|              20|                  art|\n",
      "|96bd76ec8810374ed...|                 46|                       250|                 1|             154|               18|                9|              15|       sports_leisure|\n",
      "|cef67bcfe19066a93...|                 27|                       261|                 1|             371|               26|                4|              26|                 baby|\n",
      "|9dc1a7de274444849...|                 37|                       402|                 4|             625|               20|               17|              13|           housewares|\n",
      "|41d3672d4792049fa...|                 60|                       745|                 1|             200|               38|                5|              11|  musical_instruments|\n",
      "|732bd381ad09e530f...|                 56|                      1272|                 4|           18350|               70|               24|              44|           cool_stuff|\n",
      "|2548af3e6e77a690c...|                 56|                       184|                 2|             900|               40|                8|              40|      furniture_decor|\n",
      "|37cc742be07708b53...|                 57|                       163|                 1|             400|               27|               13|              17|      home_appliances|\n",
      "|8c92109888e8cdf9d...|                 36|                      1156|                 1|             600|               17|               10|              12|                 toys|\n",
      "|14aa47b7fe5c25522...|                 54|                       630|                 1|            1100|               16|               10|              16|       bed_bath_table|\n",
      "|03b63c5fc16691530...|                 49|                       728|                 4|            7150|               50|               19|              45|                 baby|\n",
      "|cf55509ea8edaaac1...|                 43|                      1827|                 3|             250|               17|                7|              17|  musical_instruments|\n",
      "|7bb6f29c2be577161...|                 51|                      2083|                 2|             600|               68|               11|              13|      furniture_decor|\n",
      "|eb31436580a610f20...|                 59|                      1602|                 4|             200|               17|                7|              17| construction_tool...|\n",
      "|3bb7f144022e67327...|                 22|                      3021|                 1|             800|               16|                2|              11|       sports_leisure|\n",
      "|6a2fb4dd53d2cdb88...|                 39|                       346|                 2|             400|               27|                5|              20|            perfumery|\n",
      "|a1b71017a84f92fd8...|                 59|                       636|                 1|             900|               40|               15|              20| computers_accesso...|\n",
      "|a0736b92e52f6cead...|                 56|                       296|                 2|            1700|              100|                7|              15|      furniture_decor|\n",
      "|f53103a77d9cf245e...|                 52|                       206|                 1|             500|               16|               10|              16|       bed_bath_table|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a left join between the 'product_df_uncleaned' DataFrame and 'product_category_translation_df'\n",
    "# based on the 'Product_category_name' column. This operation combines the two DataFrames .\n",
    "product_joined_df= product_df_uncleaned.join(product_category_translation_df, \"Product_category_name\", \"left\")\n",
    "\n",
    "# Drop \"product_category_name\" will be removed from the DataFrame.\n",
    "product_df = product_joined_df.drop(\"product_category_name\")\n",
    "\n",
    "# Rename the \"product_category_name_english\" column to \"product_category_name\"\n",
    "product_df = product_df.withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# Show the 'product_df' DataFrame with the dropped and renamed columns.\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformation on the Dataframes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Dataframes:\n",
    "    -customer_df \n",
    "    -order_item_df \n",
    "    -order_payment_df \n",
    "    -product_category_translation_df \n",
    "    -seller_df\n",
    "    -product_df\n",
    "    -geoloacation_df\n",
    "    -orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "Create a pivot table to find the number of transactions made by customers using different payment_types for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-----------+----------+-------+\n",
      "|customer_state|boleto|credit_card|debit_card|voucher|\n",
      "+--------------+------+-----------+----------+-------+\n",
      "|            SC|   817|       2650|        45|    150|\n",
      "|            RO|    61|        180|         3|      7|\n",
      "|            PI|    89|        377|        10|     28|\n",
      "|            AM|    21|        121|         2|      7|\n",
      "|            RR|    12|         29|         0|      0|\n",
      "|            GO|   432|       1478|        22|    115|\n",
      "|            TO|    75|        192|         4|     24|\n",
      "|            MT|   234|        645|         4|     26|\n",
      "|            SP|  7952|      31244|       737|   2363|\n",
      "|            PB|    91|        412|        13|     35|\n",
      "|            ES|   399|       1540|        26|    103|\n",
      "|            RS|  1326|       3900|        74|    241|\n",
      "|            MS|   174|        506|        11|     31|\n",
      "|            AL|    64|        331|         3|     13|\n",
      "|            MG|  2243|       8858|       137|    564|\n",
      "|            PA|   205|        712|        15|     49|\n",
      "|            BA|   590|       2569|        51|    273|\n",
      "|            SE|    73|        250|         5|     16|\n",
      "|            PE|   269|       1286|        17|     95|\n",
      "|            CE|   194|       1048|        20|     78|\n",
      "+--------------+------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "orders_customer_df = orders_df.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Joined orders_customer_df with payment_df to get payment type\n",
    "orders_payment_df = orders_customer_df.join(order_payment_df, \"order_id\")\n",
    "\n",
    "# Grouped by 'customer_state' and 'payment_type' and count the orders\n",
    "pivot_table = orders_payment_df.groupBy('customer_state', 'payment_type').agg(count('order_id').alias('order_count'))\n",
    "\n",
    "# Pivoted the table to create the desired pivot table\n",
    "pivot_table = pivot_table.groupBy('customer_state').pivot('payment_type').sum('order_count')\n",
    "\n",
    "# Filled missing values with 0\n",
    "pivot_table = pivot_table.na.fill(0)\n",
    "\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "Find the total number of active sellers and how they have changed over time. This question helps to find the sellers who are actively selling their product i.e. they have not canceled their orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-------------+\n",
      "|year|active_seller|product_count|\n",
      "+----+-------------+-------------+\n",
      "|2016|          130|          241|\n",
      "|2017|         1690|        16787|\n",
      "|2018|         2331|        20184|\n",
      "+----+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, countDistinct\n",
    "\n",
    "joined_df = order_item_df.join(orders_df, order_item_df.order_id == orders_df.order_id) \\\n",
    "    .join(order_payment_df, order_item_df.order_id == order_payment_df.order_id) \\\n",
    "    .join(seller_df.alias('s'), order_item_df.seller_id == seller_df.seller_id) \\\n",
    "    .join(product_df.alias('p'), order_item_df.product_id == product_df.product_id)\n",
    "\n",
    "# Extract the year from order_approved_at column\n",
    "joined_df = joined_df.withColumn(\"year\", year(col(\"order_approved_at\")))\n",
    "\n",
    "# Calculate the counts\n",
    "result_df = joined_df.groupBy(\"year\") \\\n",
    "    .agg(countDistinct(\"s.seller_id\").alias(\"active_seller\"), countDistinct(\"p.product_id\").alias(\"product_count\"))\n",
    "\n",
    "# Order the result\n",
    "result_df = result_df.orderBy(\"year\", col(\"active_seller\").desc())\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "Find the customer shares based on the states. This question helps to provide the insights about the percentage of customers making in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:00 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 10:43:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------------------+------------------------+\n",
      "|customer_state|no_customers|percentage_customer_base|running_total_percentage|\n",
      "+--------------+------------+------------------------+------------------------+\n",
      "|            SP|       41746|                   41.98|                   41.98|\n",
      "|            RJ|       12852|                   12.92|                    54.9|\n",
      "|            MG|       11635|                    11.7|                   66.61|\n",
      "|            RS|        5466|                     5.5|                    72.1|\n",
      "|            PR|        5045|                    5.07|                   77.18|\n",
      "|            SC|        3637|                    3.66|                   80.83|\n",
      "|            BA|        3380|                     3.4|                   84.23|\n",
      "|            DF|        2140|                    2.15|                   86.38|\n",
      "|            ES|        2033|                    2.04|                   88.43|\n",
      "|            GO|        2020|                    2.03|                   90.46|\n",
      "|            PE|        1652|                    1.66|                   92.12|\n",
      "|            CE|        1336|                    1.34|                   93.46|\n",
      "|            PA|         975|                    0.98|                   94.44|\n",
      "|            MT|         907|                    0.91|                   95.36|\n",
      "|            MA|         747|                    0.75|                   96.11|\n",
      "|            MS|         715|                    0.72|                   96.83|\n",
      "|            PB|         536|                    0.54|                   97.37|\n",
      "|            PI|         495|                     0.5|                   97.86|\n",
      "|            RN|         485|                    0.49|                   98.35|\n",
      "|            AL|         413|                    0.42|                   98.77|\n",
      "+--------------+------------+------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count,desc,col,sum,round\n",
    "\n",
    "# Define a window specification for the running total calculation\n",
    "window_spec = Window.orderBy(desc(\"no_customers\"))\n",
    "\n",
    "cteDF = customer_df.groupBy(\"customer_state\") \\\n",
    "    .agg(count(\"customer_unique_id\").alias(\"no_customers\")) \\\n",
    "    .orderBy(desc(\"no_customers\")) \\\n",
    "    .withColumn(\"percentage_customer_base\", round(col(\"no_customers\") / sum(\"no_customers\").over(Window.partitionBy().orderBy())* 100, 2) ) \\\n",
    "    .withColumn(\"running_total_percentage\", round(sum(\"no_customers\").over(window_spec) / sum(\"no_customers\").over(Window.partitionBy().orderBy())* 100, 2) )\n",
    "\n",
    "resultDF = cteDF.orderBy(desc(\"no_customers\"))\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "Find the states whose sales value is higher than the buy value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -customer_df \n",
    "#     -order_item_df \n",
    "#     -order_payment_df \n",
    "#     -product_category_translation_df \n",
    "#     -seller_df\n",
    "#     -product_df\n",
    "#     -geoloacation_df\n",
    "#     -orders_df\n",
    "\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "order_item_df.createOrReplaceTempView(\"order_item\")\n",
    "order_payment_df.createOrReplaceTempView(\"order_payment\")\n",
    "geolocation_df.createOrReplaceTempView(\"location\")\n",
    "seller_df.createOrReplaceTempView(\"seller\")\n",
    "product_df.createOrReplaceTempView(\"product\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "customer_seller_comparison_SQL  = spark.sql('''SELECT a.order_id, a.price,a.seller_id,b.seller_state, c.customer_id,d.customer_state\n",
    "                                    FROM order_item a \n",
    "                                    INNER JOIN seller b ON a.seller_id = b.seller_id\n",
    "                                    INNER JOIN orders c ON a.order_id = c.order_id\n",
    "                                    INNER JOIN customer d ON d.customer_id=c.customer_id\n",
    "''')\n",
    "                                            \n",
    "# customer_seller_comparison_SQL.show()\n",
    "comparision_df = customer_seller_comparison_SQL\n",
    "\n",
    "comparision_df = comparision_df.dropDuplicates()\n",
    "\n",
    "comparision_df.count()\n",
    "\n",
    "seller_df = comparision_df.groupBy(['seller_state', 'seller_id']).agg(sum('price').alias('sell_value'))\n",
    "\n",
    "# Group by 'seller_state' and sum 'sell_value'\n",
    "seller_df = seller_df.groupBy('seller_state').agg(sum('sell_value').alias('sell_value'))\n",
    "\n",
    "# seller_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_df = comparision_df.groupBy(['customer_state', 'customer_id']).agg(sum('price').alias('buy_value'))\n",
    "\n",
    "# Group by 'customer_state' and sum 'buy_value'\n",
    "buyer_df = buyer_df.groupBy('customer_state').agg(sum('buy_value').alias('buy_value'))\n",
    "\n",
    "# buyer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "|customer_state|         buy_value|seller_state|        sell_value|    margin_activity|  margin_category|\n",
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "|            SC|469650.34999999846|          SC| 584587.8300000005| 114937.48000000208|  seller_dominant|\n",
      "|            RO| 44486.18999999998|          RO|            4762.2| -39723.98999999998|consumer_dominant|\n",
      "|            PI| 78356.66000000002|          PI|            2383.0| -75973.66000000002|consumer_dominant|\n",
      "|            AM|20835.459999999995|          AM|            1177.0|-19658.459999999995|consumer_dominant|\n",
      "|            RR|6186.5599999999995|          RR|               0.0|-6186.5599999999995|consumer_dominant|\n",
      "|            GO|259822.12000000034|          GO|60896.409999999996|-198925.71000000034|consumer_dominant|\n",
      "|            TO| 46620.56999999998|          TO|               0.0| -46620.56999999998|consumer_dominant|\n",
      "|            MT|138392.09000000023|          MT|          16578.22|-121813.87000000023|consumer_dominant|\n",
      "|            SP| 4723466.229999993|          SP|7908292.6600000495| 3184826.4300000565|  seller_dominant|\n",
      "|            ES|251124.71000000037|          ES|          41252.25|-209872.46000000037|consumer_dominant|\n",
      "|            PB| 106356.7400000001|          PB|           16327.6|  -90029.1400000001|consumer_dominant|\n",
      "|            RS| 677845.5999999929|          RS| 346799.2800000003|-331046.31999999256|consumer_dominant|\n",
      "|            MS|108995.13000000022|          MS|           8366.79|-100628.34000000023|consumer_dominant|\n",
      "|            AL| 75470.47000000002|          AL|               0.0| -75470.47000000002|consumer_dominant|\n",
      "|            MG|1451489.4700000044|          MG| 906240.8900000004|  -545248.580000004|consumer_dominant|\n",
      "|            PA|165959.72000000032|          PA|            1238.0|-164721.72000000032|consumer_dominant|\n",
      "|            BA| 455765.6099999988|          BA|258648.02000000005|-197117.58999999877|consumer_dominant|\n",
      "|            SE|52805.139999999985|          SE|1417.1999999999998| -51387.93999999999|consumer_dominant|\n",
      "|            PE|240033.51000000018|          PE| 86042.07999999999| -153991.4300000002|consumer_dominant|\n",
      "|            CE|206419.72000000026|          CE|19271.940000000006|-187147.78000000026|consumer_dominant|\n",
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Join buyer_df and seller_df on 'customer_state' and 'seller_state'\n",
    "compare_buy_sell_activity = buyer_df.alias(\"buyer\").join(\n",
    "    seller_df.alias(\"seller\"),\n",
    "    col(\"buyer.customer_state\") == col(\"seller.seller_state\"),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values with 0 for 'sell_value' and 'seller_state'\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.fillna(0, subset=[\"sell_value\"])\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"seller_state\",\n",
    "    col(\"buyer.customer_state\")\n",
    ")\n",
    "\n",
    "# Calculate the margin activity\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"margin_activity\",\n",
    "    compare_buy_sell_activity[\"sell_value\"] - compare_buy_sell_activity[\"buy_value\"]\n",
    ")\n",
    "\n",
    "def encode_margin_udf(value):\n",
    "    if value < 0:\n",
    "        return 'consumer_dominant'\n",
    "    elif value == 0:\n",
    "        return 'balanced'\n",
    "    elif value > 0:\n",
    "        return 'seller_dominant'\n",
    "\n",
    "# Register the UDF\n",
    "encode_margin_udf_spark = udf(encode_margin_udf, StringType())\n",
    "\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"margin_category\",\n",
    "    encode_margin_udf_spark(compare_buy_sell_activity[\"margin_activity\"])\n",
    ")\n",
    "\n",
    "compare_buy_sell_activity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5\n",
    "Find the total number of orders placed by customers every hour in each day of week .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2017-10-02 10:56:33|e481f51cbdc54678b...|\n",
      "|     2018-07-24 20:41:37|53cdb2fc8bc7dce0b...|\n",
      "|     2018-08-08 08:38:49|47770eb9100c2d0c4...|\n",
      "|     2017-11-18 19:28:06|949d5b44dbf5de918...|\n",
      "|     2018-02-13 21:18:39|ad21c59c0840e6cb8...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_result_order = orders_df.select(\"order_purchase_timestamp\", \"order_id\")\n",
    "query_result_order.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Checking\n",
      "Number of null values in each column:\n",
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2017-10-02 10:56:33|e481f51cbdc54678b...|\n",
      "|     2018-07-24 20:41:37|53cdb2fc8bc7dce0b...|\n",
      "|     2018-08-08 08:38:49|47770eb9100c2d0c4...|\n",
      "|     2017-11-18 19:28:06|949d5b44dbf5de918...|\n",
      "|     2018-02-13 21:18:39|ad21c59c0840e6cb8...|\n",
      "|     2017-07-09 21:57:05|a4591c265e18cb1dc...|\n",
      "|     2017-05-16 13:10:30|6514b8ad8028c9f2c...|\n",
      "|     2017-01-23 18:29:09|76c6e866289321a7c...|\n",
      "|     2017-07-29 11:55:02|e69bfb5eb88e0ed6a...|\n",
      "|     2017-05-16 19:41:10|e6ce16cb79ec1d90b...|\n",
      "|     2017-07-13 19:58:11|34513ce0c4fab462a...|\n",
      "|     2018-06-07 10:06:19|82566a660a982b15f...|\n",
      "|     2018-07-25 17:44:10|5ff96c15d0b717ac6...|\n",
      "|     2018-03-01 14:14:28|432aaf21d85167c2c...|\n",
      "|     2018-06-07 19:03:12|dcb36b511fcac050b...|\n",
      "|     2018-01-02 19:00:43|403b97836b0c04a62...|\n",
      "|     2017-12-26 23:41:31|116f0b09343b49556...|\n",
      "|     2017-11-21 00:03:41|85ce859fd6dc634de...|\n",
      "|     2017-10-26 15:54:26|83018ec114eee8641...|\n",
      "|     2017-09-18 14:31:30|203096f03d82e0dff...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total null values: 0\n",
      "Duplicate Data Checking\n",
      "Number of duplicate rows: 0\n",
      "After Removing Duplicate Data\n",
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2018-01-11 16:51:17|a0d5b8474423ddf55...|\n",
      "|     2017-09-20 11:35:09|9f035a4126d6f9cf4...|\n",
      "|     2018-01-11 21:42:30|dd6d0f11a9c3d2abd...|\n",
      "|     2017-10-27 16:44:14|5711d8a02451fd508...|\n",
      "|     2018-03-10 16:15:17|4f32c93f66aadbd68...|\n",
      "|     2017-05-31 11:02:39|d2f2458326a050b12...|\n",
      "|     2018-01-07 19:59:42|3ecb666eda6d2ebb9...|\n",
      "|     2017-06-19 08:03:22|210d1686d116f7589...|\n",
      "|     2018-08-28 21:56:30|dbb786f88b6d4e52f...|\n",
      "|     2017-11-03 03:03:15|79aa91e33030a170c...|\n",
      "|     2018-01-12 21:59:20|3fa59277573f0fe06...|\n",
      "|     2018-02-10 09:25:05|44a10a741d51010c1...|\n",
      "|     2017-08-06 21:38:10|caa157284ffa71411...|\n",
      "|     2018-07-09 14:31:00|fb0a15c6b3645581d...|\n",
      "|     2017-09-05 09:29:17|a1fa82769a203e30b...|\n",
      "|     2018-03-03 09:57:04|e639eb028398f98b7...|\n",
      "|     2018-08-07 14:10:05|69b4682d3ab5ef1f1...|\n",
      "|     2018-04-11 10:45:45|efc21a26212414ea2...|\n",
      "|     2018-01-08 19:38:58|8b21fb2df5ea38807...|\n",
      "|     2017-10-26 10:13:29|b32fad437a7904a2d...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Null Checking\")\n",
    "null_counts = query_result_order.select([col(c).alias(c) for c in query_result_order.columns]).na.drop().count()\n",
    "print(\"Number of null values in each column:\")\n",
    "query_result_order.select([col(c).alias(c) for c in query_result_order.columns]).na.drop().show()\n",
    "print(f\"Total null values: {query_result_order.count() - null_counts}\")\n",
    "\n",
    "# Duplicate data checking\n",
    "print(\"Duplicate Data Checking\")\n",
    "duplicate_count = query_result_order.count() - query_result_order.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Drop duplicate data\n",
    "query_result_order = query_result_order.dropDuplicates()\n",
    "print(\"After Removing Duplicate Data\")\n",
    "query_result_order.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes:\n",
      "root\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      "\n",
      "Dtypes After Conversion:\n",
      "root\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col,date_format,hour\n",
    "\n",
    "print(\"Dtypes:\")\n",
    "query_result_order.printSchema()\n",
    "\n",
    "# Convert the 'order_purchase_timestamp' column to a timestamp type\n",
    "query_result_order = query_result_order.withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Parse day name\n",
    "query_result_order = query_result_order.withColumn(\"Day\", date_format(col(\"order_purchase_timestamp\"), \"EEEE\"))\n",
    "\n",
    "# Parse hour\n",
    "query_result_order = query_result_order.withColumn(\"Hour\", hour(col(\"order_purchase_timestamp\")))\n",
    "\n",
    "# Check data types after conversion\n",
    "print(\"Dtypes After Conversion:\")\n",
    "query_result_order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "|Hour|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|\n",
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "|  23|   697|    676|      600|     532|   496|     422|   591|\n",
      "|  22|   962|    939|      855|     834|   675|     534|   857|\n",
      "|  21|  1084|    996|      927|     826|   700|     645|   862|\n",
      "|  20|   998|    946|      881|     816|   714|     709|   944|\n",
      "|  19|   921|    894|      822|     798|   757|     729|   881|\n",
      "|  18|   891|    852|      818|     759|   697|     693|   875|\n",
      "|  17|   954|    942|      943|     881|   790|     673|   776|\n",
      "|  16|  1060|   1059|     1007|    1042|   943|     675|   688|\n",
      "|  15|  1047|   1005|      952|     901|   947|     699|   697|\n",
      "|  14|  1063|   1095|     1027|     951|   938|     649|   660|\n",
      "|  13|  1000|   1011|      990|     953|   967|     685|   701|\n",
      "|  12|   944|    874|      897|     939|   827|     666|   653|\n",
      "|  11|  1052|   1023|     1030|     928|   960|     686|   705|\n",
      "|  10|   999|    940|     1007|     956|   928|     635|   513|\n",
      "|   9|   752|    845|      798|     736|   754|     421|   341|\n",
      "|   8|   471|    510|      497|     495|   486|     245|   202|\n",
      "|   7|   157|    216|      208|     211|   203|     102|   102|\n",
      "|   6|    64|     69|       90|      82|    91|      51|    30|\n",
      "|   5|    22|     24|       24|      28|    35|      23|    26|\n",
      "|   4|    21|     28|       33|      30|    39|      25|    27|\n",
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "custom_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Use the 'when' function to create a custom sorting column\n",
    "day_hour_group = query_result_order.groupBy('Hour').pivot('Day').agg(count('order_id'))\n",
    "day_hour_group = day_hour_group.select(\n",
    "    ['Hour'] + [F.col(day).alias(day) for day in custom_order]\n",
    ")\n",
    "\n",
    "# Order the DataFrame by 'Hour'\n",
    "day_hour_group = day_hour_group.orderBy('Hour', ascending=False)\n",
    "\n",
    "# Show the result\n",
    "day_hour_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

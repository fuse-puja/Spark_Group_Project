{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining path to the dataset\n",
    "customer_data_path = \"./Data/olist_customers_dataset.csv\"  # Replace with the actual path\n",
    "order_item_path = \"./Data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"./Data/olist_order_payments_dataset.csv\"\n",
    "product_category_translation_path= \"./Data/product_category_name_translation.csv\"\n",
    "product_path = './Data/olist_products_dataset.csv'\n",
    "seller_path = './Data/olist_sellers_dataset.csv'\n",
    "geolocation_path = './Data/olist_geolocation_dataset.csv'\n",
    "orders_path = './Data/olist_orders_dataset.csv'\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "customer_df = spark.read.csv(customer_data_path, header=True, inferSchema=True)\n",
    "order_item_df = spark.read.csv(order_item_path, header=True, inferSchema=True)\n",
    "order_payment_df = spark.read.csv(order_payment_path, header=True, inferSchema=True)\n",
    "product_category_translation_df = spark.read.csv(product_category_translation_path, header=True, inferSchema=True)\n",
    "seller_df_uncleaned = spark.read.csv(seller_path, header=True, inferSchema=True)\n",
    "product_df_uncleaned = spark.read.csv(product_path, header=True, inferSchema=True)\n",
    "geolocation_df_uncleaned = spark.read.csv(geolocation_path, header=True, inferSchema= True)\n",
    "orders_df_uncleaned = spark.read.csv(orders_path, header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim,regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "seller_df_uncleaned.select([trim(col(c)).alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "# Remove whitespace characters between words in all columns\n",
    "seller_df = seller_df_uncleaned.select([regexp_replace(col(c), r'\\s+', ' ').alias(c) for c in seller_df_uncleaned.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       são paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       são paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       são paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "geolocation_df_uncleaned.select([trim(col(c)).alias(c) for c in geoloacation_df_uncleaned.columns])\n",
    "\n",
    "geolocation_df_uncleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       sao paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       sao paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace \"são paulo\" with \"sao paulo\" in the geolocation dataframe\n",
    "geolocation_df = geoloacation_df_uncleaned.replace(\"são paulo\", \"sao paulo\")\n",
    "\n",
    "# Show the DataFrame with the replaced values\n",
    "geolocation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows in uncleaned dataset =  99441\n",
      "No of rows of cleaned datset =  96461\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the 'orders_df_uncleaned' DataFrame\n",
    "print(\"No of rows in uncleaned dataset = \", orders_df_uncleaned.count())\n",
    "\n",
    "# Drop rows with null values in the 'orders_df_uncleaned' DataFrame\n",
    "orders_df = orders_df_uncleaned.dropna()\n",
    "\n",
    "# Print the number of rows in the 'orders_df' DataFrame after dropping null values\n",
    "print(\"No of rows of cleaned datset = \", orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing column on product dataset with content from product category translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|          product_id|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_category_name|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|1e9e8ef04dbcff454...|                 40|                       287|                 1|             225|               16|               10|              14|            perfumery|\n",
      "|3aa071139cb16b67c...|                 44|                       276|                 1|            1000|               30|               18|              20|                  art|\n",
      "|96bd76ec8810374ed...|                 46|                       250|                 1|             154|               18|                9|              15|       sports_leisure|\n",
      "|cef67bcfe19066a93...|                 27|                       261|                 1|             371|               26|                4|              26|                 baby|\n",
      "|9dc1a7de274444849...|                 37|                       402|                 4|             625|               20|               17|              13|           housewares|\n",
      "|41d3672d4792049fa...|                 60|                       745|                 1|             200|               38|                5|              11|  musical_instruments|\n",
      "|732bd381ad09e530f...|                 56|                      1272|                 4|           18350|               70|               24|              44|           cool_stuff|\n",
      "|2548af3e6e77a690c...|                 56|                       184|                 2|             900|               40|                8|              40|      furniture_decor|\n",
      "|37cc742be07708b53...|                 57|                       163|                 1|             400|               27|               13|              17|      home_appliances|\n",
      "|8c92109888e8cdf9d...|                 36|                      1156|                 1|             600|               17|               10|              12|                 toys|\n",
      "|14aa47b7fe5c25522...|                 54|                       630|                 1|            1100|               16|               10|              16|       bed_bath_table|\n",
      "|03b63c5fc16691530...|                 49|                       728|                 4|            7150|               50|               19|              45|                 baby|\n",
      "|cf55509ea8edaaac1...|                 43|                      1827|                 3|             250|               17|                7|              17|  musical_instruments|\n",
      "|7bb6f29c2be577161...|                 51|                      2083|                 2|             600|               68|               11|              13|      furniture_decor|\n",
      "|eb31436580a610f20...|                 59|                      1602|                 4|             200|               17|                7|              17| construction_tool...|\n",
      "|3bb7f144022e67327...|                 22|                      3021|                 1|             800|               16|                2|              11|       sports_leisure|\n",
      "|6a2fb4dd53d2cdb88...|                 39|                       346|                 2|             400|               27|                5|              20|            perfumery|\n",
      "|a1b71017a84f92fd8...|                 59|                       636|                 1|             900|               40|               15|              20| computers_accesso...|\n",
      "|a0736b92e52f6cead...|                 56|                       296|                 2|            1700|              100|                7|              15|      furniture_decor|\n",
      "|f53103a77d9cf245e...|                 52|                       206|                 1|             500|               16|               10|              16|       bed_bath_table|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a left join between the 'product_df_uncleaned' DataFrame and 'product_category_translation_df'\n",
    "# based on the 'Product_category_name' column. This operation combines the two DataFrames .\n",
    "product_joined_df= product_df_uncleaned.join(product_category_translation_df, \"Product_category_name\", \"left\")\n",
    "\n",
    "# Drop \"product_category_name\" will be removed from the DataFrame.\n",
    "product_df = product_joined_df.drop(\"product_category_name\")\n",
    "\n",
    "# Rename the \"product_category_name_english\" column to \"product_category_name\"\n",
    "product_df = product_df.withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# Show the 'product_df' DataFrame with the dropped and renamed columns.\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformation on the Dataframes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Dataframes:\n",
    "    -customer_df \n",
    "    -order_item_df \n",
    "    -order_payment_df \n",
    "    -product_category_translation_df \n",
    "    -seller_df\n",
    "    -product_df\n",
    "    -geolocation_df\n",
    "    -orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, avg\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+\n",
      "|Payment_type| Total Payment Value| Avg Installments|\n",
      "+------------+--------------------+-----------------+\n",
      "|      boleto|  2869361.2700000196|              1.0|\n",
      "| credit_card|1.2542084189999647E7|3.507155413763917|\n",
      "|  debit_card|  217989.79000000015|              1.0|\n",
      "| not_defined|                 0.0|              1.0|\n",
      "|     voucher|   379436.8700000001|              1.0|\n",
      "+------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_analysis = order_payment_df.groupBy(\"Payment_type\") \\\n",
    "    .agg(\n",
    "        sum(\"Payment_value\").alias(\"Total Payment Value\"),\n",
    "        avg(\"Payment_installments\").alias(\"Avg Installments\")\n",
    "    ) \\\n",
    "    .orderBy(\"Payment_type\")\n",
    "\n",
    "payment_analysis.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

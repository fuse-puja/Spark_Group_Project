{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining path to the dataset\n",
    "customer_data_path = \"./Data/olist_customers_dataset.csv\"  # Replace with the actual path\n",
    "order_item_path = \"./Data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"./Data/olist_order_payments_dataset.csv\"\n",
    "product_category_translation_path= \"./Data/product_category_name_translation.csv\"\n",
    "product_path = './Data/olist_products_dataset.csv'\n",
    "seller_path = './Data/olist_sellers_dataset.csv'\n",
    "geolocation_path = './Data/olist_geolocation_dataset.csv'\n",
    "orders_path = './Data/olist_orders_dataset.csv'\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "customer_df = spark.read.csv(customer_data_path, header=True, inferSchema=True)\n",
    "order_item_df = spark.read.csv(order_item_path, header=True, inferSchema=True)\n",
    "order_payment_df = spark.read.csv(order_payment_path, header=True, inferSchema=True)\n",
    "product_category_translation_df = spark.read.csv(product_category_translation_path, header=True, inferSchema=True)\n",
    "seller_df_uncleaned = spark.read.csv(seller_path, header=True, inferSchema=True)\n",
    "product_df_uncleaned = spark.read.csv(product_path, header=True, inferSchema=True)\n",
    "geoloacation_df_uncleaned = spark.read.csv(geolocation_path, header=True, inferSchema= True)\n",
    "orders_df_uncleaned = spark.read.csv(orders_path, header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim,regexp_replace, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "seller_df_uncleaned.select([trim(col(c)).alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "# Remove whitespace characters between words in all columns\n",
    "seller_df = seller_df_uncleaned.select([regexp_replace(col(c), r'\\s+', ' ').alias(c) for c in seller_df_uncleaned.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       são paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       são paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       são paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "geoloacation_df_uncleaned.select([trim(col(c)).alias(c) for c in geoloacation_df_uncleaned.columns])\n",
    "\n",
    "geoloacation_df_uncleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       sao paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       sao paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace \"são paulo\" with \"sao paulo\" in the geolocation dataframe\n",
    "geolocation_df = geoloacation_df_uncleaned.replace(\"são paulo\", \"sao paulo\")\n",
    "\n",
    "# Show the DataFrame with the replaced values\n",
    "geolocation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows in uncleaned dataset =  99441\n",
      "No of rows of cleaned datset =  96461\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the 'orders_df_uncleaned' DataFrame\n",
    "print(\"No of rows in uncleaned dataset = \", orders_df_uncleaned.count())\n",
    "\n",
    "# Drop rows with null values in the 'orders_df_uncleaned' DataFrame\n",
    "orders_df = orders_df_uncleaned.dropna()\n",
    "\n",
    "# Print the number of rows in the 'orders_df' DataFrame after dropping null values\n",
    "print(\"No of rows of cleaned datset = \", orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing column on product dataset with content from product category translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|          product_id|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_category_name|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|1e9e8ef04dbcff454...|                 40|                       287|                 1|             225|               16|               10|              14|            perfumery|\n",
      "|3aa071139cb16b67c...|                 44|                       276|                 1|            1000|               30|               18|              20|                  art|\n",
      "|96bd76ec8810374ed...|                 46|                       250|                 1|             154|               18|                9|              15|       sports_leisure|\n",
      "|cef67bcfe19066a93...|                 27|                       261|                 1|             371|               26|                4|              26|                 baby|\n",
      "|9dc1a7de274444849...|                 37|                       402|                 4|             625|               20|               17|              13|           housewares|\n",
      "|41d3672d4792049fa...|                 60|                       745|                 1|             200|               38|                5|              11|  musical_instruments|\n",
      "|732bd381ad09e530f...|                 56|                      1272|                 4|           18350|               70|               24|              44|           cool_stuff|\n",
      "|2548af3e6e77a690c...|                 56|                       184|                 2|             900|               40|                8|              40|      furniture_decor|\n",
      "|37cc742be07708b53...|                 57|                       163|                 1|             400|               27|               13|              17|      home_appliances|\n",
      "|8c92109888e8cdf9d...|                 36|                      1156|                 1|             600|               17|               10|              12|                 toys|\n",
      "|14aa47b7fe5c25522...|                 54|                       630|                 1|            1100|               16|               10|              16|       bed_bath_table|\n",
      "|03b63c5fc16691530...|                 49|                       728|                 4|            7150|               50|               19|              45|                 baby|\n",
      "|cf55509ea8edaaac1...|                 43|                      1827|                 3|             250|               17|                7|              17|  musical_instruments|\n",
      "|7bb6f29c2be577161...|                 51|                      2083|                 2|             600|               68|               11|              13|      furniture_decor|\n",
      "|eb31436580a610f20...|                 59|                      1602|                 4|             200|               17|                7|              17| construction_tool...|\n",
      "|3bb7f144022e67327...|                 22|                      3021|                 1|             800|               16|                2|              11|       sports_leisure|\n",
      "|6a2fb4dd53d2cdb88...|                 39|                       346|                 2|             400|               27|                5|              20|            perfumery|\n",
      "|a1b71017a84f92fd8...|                 59|                       636|                 1|             900|               40|               15|              20| computers_accesso...|\n",
      "|a0736b92e52f6cead...|                 56|                       296|                 2|            1700|              100|                7|              15|      furniture_decor|\n",
      "|f53103a77d9cf245e...|                 52|                       206|                 1|             500|               16|               10|              16|       bed_bath_table|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a left join between the 'product_df_uncleaned' DataFrame and 'product_category_translation_df'\n",
    "# based on the 'Product_category_name' column. This operation combines the two DataFrames .\n",
    "product_joined_df= product_df_uncleaned.join(product_category_translation_df, \"Product_category_name\", \"left\")\n",
    "\n",
    "# Drop \"product_category_name\" will be removed from the DataFrame.\n",
    "product_df = product_joined_df.drop(\"product_category_name\")\n",
    "\n",
    "# Rename the \"product_category_name_english\" column to \"product_category_name\"\n",
    "product_df = product_df.withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# Show the 'product_df' DataFrame with the dropped and renamed columns.\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set payment_installment to 0 where payment_type is \"not_defined\"\n",
    "order_payment_df = order_payment_df.withColumn(\"Payment_installments\",\n",
    "                                   when(col(\"Payment_type\") == \"not_defined\", 0)\n",
    "                                   .otherwise(col(\"Payment_installments\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformation on the Dataframes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Dataframes:\n",
    "    -customer_df \n",
    "    -order_item_df \n",
    "    -order_payment_df \n",
    "    -product_category_translation_df \n",
    "    -seller_df\n",
    "    -product_df\n",
    "    -geoloacation_df\n",
    "    -orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pivot table to find the number of transactions made by customers using different payment_types for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-----------+----------+-------+\n",
      "|customer_state|boleto|credit_card|debit_card|voucher|\n",
      "+--------------+------+-----------+----------+-------+\n",
      "|            SC|   817|       2650|        45|    150|\n",
      "|            RO|    61|        180|         3|      7|\n",
      "|            PI|    89|        377|        10|     28|\n",
      "|            AM|    21|        121|         2|      7|\n",
      "|            RR|    12|         29|         0|      0|\n",
      "|            GO|   432|       1478|        22|    115|\n",
      "|            TO|    75|        192|         4|     24|\n",
      "|            MT|   234|        645|         4|     26|\n",
      "|            SP|  7952|      31244|       737|   2363|\n",
      "|            PB|    91|        412|        13|     35|\n",
      "|            ES|   399|       1540|        26|    103|\n",
      "|            RS|  1326|       3900|        74|    241|\n",
      "|            MS|   174|        506|        11|     31|\n",
      "|            AL|    64|        331|         3|     13|\n",
      "|            MG|  2243|       8858|       137|    564|\n",
      "|            PA|   205|        712|        15|     49|\n",
      "|            BA|   590|       2569|        51|    273|\n",
      "|            SE|    73|        250|         5|     16|\n",
      "|            PE|   269|       1286|        17|     95|\n",
      "|            CE|   194|       1048|        20|     78|\n",
      "+--------------+------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "orders_customer_df = orders_df.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Joined orders_customer_df with payment_df to get payment type\n",
    "orders_payment_df = orders_customer_df.join(order_payment_df, \"order_id\")\n",
    "\n",
    "# Grouped by 'customer_state' and 'payment_type' and count the orders\n",
    "pivot_table = orders_payment_df.groupBy('customer_state', 'payment_type').agg(count('order_id').alias('order_count'))\n",
    "\n",
    "# Pivoted the table to create the desired pivot table\n",
    "pivot_table = pivot_table.groupBy('customer_state').pivot('payment_type').sum('order_count')\n",
    "\n",
    "# Filled missing values with 0\n",
    "pivot_table = pivot_table.na.fill(0)\n",
    "\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find the total number of active sellers and how they have changed over time. This question helps to find the sellers who are actively selling their product i.e. they have not canceled their orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-------------+\n",
      "|year|active_seller|product_count|\n",
      "+----+-------------+-------------+\n",
      "|2016|          130|          241|\n",
      "|2017|         1690|        16787|\n",
      "|2018|         2331|        20184|\n",
      "+----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, countDistinct\n",
    "\n",
    "joined_df = order_item_df.join(orders_df, order_item_df.order_id == orders_df.order_id) \\\n",
    "    .join(order_payment_df, order_item_df.order_id == order_payment_df.order_id) \\\n",
    "    .join(seller_df.alias('s'), order_item_df.seller_id == seller_df.seller_id) \\\n",
    "    .join(product_df.alias('p'), order_item_df.product_id == product_df.product_id)\n",
    "\n",
    "# Extract the year from order_approved_at column\n",
    "joined_df = joined_df.withColumn(\"year\", year(col(\"order_approved_at\")))\n",
    "\n",
    "# Calculate the counts\n",
    "result_df = joined_df.groupBy(\"year\") \\\n",
    "    .agg(countDistinct(\"s.seller_id\").alias(\"active_seller\"), countDistinct(\"p.product_id\").alias(\"product_count\"))\n",
    "\n",
    "# Order the result\n",
    "result_df = result_df.orderBy(\"year\", col(\"active_seller\").desc())\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the customer shares based on the states. This question helps to provide the insights about the percentage of customers making in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------------------+------------------------+\n",
      "|customer_state|no_customers|percentage_customer_base|running_total_percentage|\n",
      "+--------------+------------+------------------------+------------------------+\n",
      "|            SP|       41746|                   41.98|                   41.98|\n",
      "|            RJ|       12852|                   12.92|                    54.9|\n",
      "|            MG|       11635|                    11.7|                   66.61|\n",
      "|            RS|        5466|                     5.5|                    72.1|\n",
      "|            PR|        5045|                    5.07|                   77.18|\n",
      "|            SC|        3637|                    3.66|                   80.83|\n",
      "|            BA|        3380|                     3.4|                   84.23|\n",
      "|            DF|        2140|                    2.15|                   86.38|\n",
      "|            ES|        2033|                    2.04|                   88.43|\n",
      "|            GO|        2020|                    2.03|                   90.46|\n",
      "|            PE|        1652|                    1.66|                   92.12|\n",
      "|            CE|        1336|                    1.34|                   93.46|\n",
      "|            PA|         975|                    0.98|                   94.44|\n",
      "|            MT|         907|                    0.91|                   95.36|\n",
      "|            MA|         747|                    0.75|                   96.11|\n",
      "|            MS|         715|                    0.72|                   96.83|\n",
      "|            PB|         536|                    0.54|                   97.37|\n",
      "|            PI|         495|                     0.5|                   97.86|\n",
      "|            RN|         485|                    0.49|                   98.35|\n",
      "|            AL|         413|                    0.42|                   98.77|\n",
      "+--------------+------------+------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/06 14:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count,desc,col,sum,round\n",
    "\n",
    "# Define a window specification for the running total calculation\n",
    "window_spec = Window.orderBy(desc(\"no_customers\"))\n",
    "\n",
    "cteDF = customer_df.groupBy(\"customer_state\") \\\n",
    "    .agg(count(\"customer_unique_id\").alias(\"no_customers\")) \\\n",
    "    .orderBy(desc(\"no_customers\")) \\\n",
    "    .withColumn(\"percentage_customer_base\", round(col(\"no_customers\") / sum(\"no_customers\").over(Window.partitionBy().orderBy())* 100, 2) ) \\\n",
    "    .withColumn(\"running_total_percentage\", round(sum(\"no_customers\").over(window_spec) / sum(\"no_customers\").over(Window.partitionBy().orderBy())* 100, 2) )\n",
    "\n",
    "resultDF = cteDF.orderBy(desc(\"no_customers\"))\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the states whose sales value is higher than the buy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "order_item_df.createOrReplaceTempView(\"order_item\")\n",
    "order_payment_df.createOrReplaceTempView(\"order_payment\")\n",
    "geolocation_df.createOrReplaceTempView(\"location\")\n",
    "seller_df.createOrReplaceTempView(\"seller\")\n",
    "product_df.createOrReplaceTempView(\"product\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "customer_seller_comparison_SQL  = spark.sql('''SELECT a.order_id, a.price,a.seller_id,b.seller_state, c.customer_id,d.customer_state\n",
    "                                    FROM order_item a \n",
    "                                    INNER JOIN seller b ON a.seller_id = b.seller_id\n",
    "                                    INNER JOIN orders c ON a.order_id = c.order_id\n",
    "                                    INNER JOIN customer d ON d.customer_id=c.customer_id\n",
    "''')\n",
    "                                            \n",
    "# customer_seller_comparison_SQL.show()\n",
    "comparision_df = customer_seller_comparison_SQL\n",
    "\n",
    "comparision_df = comparision_df.dropDuplicates()\n",
    "\n",
    "comparision_df.count()\n",
    "\n",
    "seller_df = comparision_df.groupBy(['seller_state', 'seller_id']).agg(sum('price').alias('sell_value'))\n",
    "\n",
    "# Group by 'seller_state' and sum 'sell_value'\n",
    "seller_df = seller_df.groupBy('seller_state').agg(sum('sell_value').alias('sell_value'))\n",
    "\n",
    "# seller_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_df = comparision_df.groupBy(['customer_state', 'customer_id']).agg(sum('price').alias('buy_value'))\n",
    "\n",
    "# Group by 'customer_state' and sum 'buy_value'\n",
    "buyer_df = buyer_df.groupBy('customer_state').agg(sum('buy_value').alias('buy_value'))\n",
    "\n",
    "# buyer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1154:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "|customer_state|         buy_value|seller_state|        sell_value|    margin_activity|  margin_category|\n",
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "|            SC|469650.34999999905|          SC| 584587.8300000004| 114937.48000000138|  seller_dominant|\n",
      "|            RO| 44486.18999999998|          RO|            4762.2| -39723.98999999998|consumer_dominant|\n",
      "|            PI|          78356.66|          PI|            2383.0|          -75973.66|consumer_dominant|\n",
      "|            AM|20835.459999999995|          AM|            1177.0|-19658.459999999995|consumer_dominant|\n",
      "|            RR|6186.5599999999995|          RR|               0.0|-6186.5599999999995|consumer_dominant|\n",
      "|            GO| 259822.1200000004|          GO|          60896.41| -198925.7100000004|consumer_dominant|\n",
      "|            TO| 46620.56999999998|          TO|               0.0| -46620.56999999998|consumer_dominant|\n",
      "|            MT| 138392.0900000003|          MT|          16578.22|-121813.87000000029|consumer_dominant|\n",
      "|            SP| 4723466.229999991|          SP| 7908292.660000046| 3184826.4300000547|  seller_dominant|\n",
      "|            ES| 251124.7100000005|          ES|          41252.25| -209872.4600000005|consumer_dominant|\n",
      "|            PB|106356.74000000014|          PB|           16327.6| -90029.14000000013|consumer_dominant|\n",
      "|            RS| 677845.5999999931|          RS| 346799.2800000002| -331046.3199999929|consumer_dominant|\n",
      "|            MS|108995.13000000018|          MS|           8366.79|-100628.34000000017|consumer_dominant|\n",
      "|            AL|          75470.47|          AL|               0.0|          -75470.47|consumer_dominant|\n",
      "|            MG|1451489.4700000044|          MG| 906240.8900000005| -545248.5800000039|consumer_dominant|\n",
      "|            PA| 165959.7200000003|          PA|            1238.0| -164721.7200000003|consumer_dominant|\n",
      "|            BA| 455765.6099999992|          BA|258648.02000000005|-197117.58999999912|consumer_dominant|\n",
      "|            SE|52805.139999999956|          SE|            1417.2| -51387.93999999996|consumer_dominant|\n",
      "|            PE|240033.51000000033|          PE| 86042.07999999993| -153991.4300000004|consumer_dominant|\n",
      "|            CE|206419.72000000032|          CE|19271.940000000006|-187147.78000000032|consumer_dominant|\n",
      "+--------------+------------------+------------+------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Join buyer_df and seller_df on 'customer_state' and 'seller_state'\n",
    "compare_buy_sell_activity = buyer_df.alias(\"buyer\").join(\n",
    "    seller_df.alias(\"seller\"),\n",
    "    col(\"buyer.customer_state\") == col(\"seller.seller_state\"),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values with 0 for 'sell_value' and 'seller_state'\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.fillna(0, subset=[\"sell_value\"])\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"seller_state\",\n",
    "    col(\"buyer.customer_state\")\n",
    ")\n",
    "\n",
    "# Calculate the margin activity\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"margin_activity\",\n",
    "    compare_buy_sell_activity[\"sell_value\"] - compare_buy_sell_activity[\"buy_value\"]\n",
    ")\n",
    "\n",
    "def encode_margin_udf(value):\n",
    "    if value < 0:\n",
    "        return 'consumer_dominant'\n",
    "    elif value == 0:\n",
    "        return 'balanced'\n",
    "    elif value > 0:\n",
    "        return 'seller_dominant'\n",
    "\n",
    "# Register the UDF\n",
    "encode_margin_udf_spark = udf(encode_margin_udf, StringType())\n",
    "\n",
    "compare_buy_sell_activity = compare_buy_sell_activity.withColumn(\n",
    "    \"margin_category\",\n",
    "    encode_margin_udf_spark(compare_buy_sell_activity[\"margin_activity\"])\n",
    ")\n",
    "\n",
    "compare_buy_sell_activity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the total number of orders placed by customers every hour in each day of week .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2017-10-02 10:56:33|e481f51cbdc54678b...|\n",
      "|     2018-07-24 20:41:37|53cdb2fc8bc7dce0b...|\n",
      "|     2018-08-08 08:38:49|47770eb9100c2d0c4...|\n",
      "|     2017-11-18 19:28:06|949d5b44dbf5de918...|\n",
      "|     2018-02-13 21:18:39|ad21c59c0840e6cb8...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_result_order = orders_df.select(\"order_purchase_timestamp\", \"order_id\")\n",
    "query_result_order.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Checking\n",
      "Number of null values in each column:\n",
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2017-10-02 10:56:33|e481f51cbdc54678b...|\n",
      "|     2018-07-24 20:41:37|53cdb2fc8bc7dce0b...|\n",
      "|     2018-08-08 08:38:49|47770eb9100c2d0c4...|\n",
      "|     2017-11-18 19:28:06|949d5b44dbf5de918...|\n",
      "|     2018-02-13 21:18:39|ad21c59c0840e6cb8...|\n",
      "|     2017-07-09 21:57:05|a4591c265e18cb1dc...|\n",
      "|     2017-05-16 13:10:30|6514b8ad8028c9f2c...|\n",
      "|     2017-01-23 18:29:09|76c6e866289321a7c...|\n",
      "|     2017-07-29 11:55:02|e69bfb5eb88e0ed6a...|\n",
      "|     2017-05-16 19:41:10|e6ce16cb79ec1d90b...|\n",
      "|     2017-07-13 19:58:11|34513ce0c4fab462a...|\n",
      "|     2018-06-07 10:06:19|82566a660a982b15f...|\n",
      "|     2018-07-25 17:44:10|5ff96c15d0b717ac6...|\n",
      "|     2018-03-01 14:14:28|432aaf21d85167c2c...|\n",
      "|     2018-06-07 19:03:12|dcb36b511fcac050b...|\n",
      "|     2018-01-02 19:00:43|403b97836b0c04a62...|\n",
      "|     2017-12-26 23:41:31|116f0b09343b49556...|\n",
      "|     2017-11-21 00:03:41|85ce859fd6dc634de...|\n",
      "|     2017-10-26 15:54:26|83018ec114eee8641...|\n",
      "|     2017-09-18 14:31:30|203096f03d82e0dff...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total null values: 0\n",
      "Duplicate Data Checking\n",
      "Number of duplicate rows: 0\n",
      "After Removing Duplicate Data\n",
      "+------------------------+--------------------+\n",
      "|order_purchase_timestamp|            order_id|\n",
      "+------------------------+--------------------+\n",
      "|     2018-01-11 16:51:17|a0d5b8474423ddf55...|\n",
      "|     2017-09-20 11:35:09|9f035a4126d6f9cf4...|\n",
      "|     2018-01-11 21:42:30|dd6d0f11a9c3d2abd...|\n",
      "|     2017-10-27 16:44:14|5711d8a02451fd508...|\n",
      "|     2018-03-10 16:15:17|4f32c93f66aadbd68...|\n",
      "|     2017-05-31 11:02:39|d2f2458326a050b12...|\n",
      "|     2018-01-07 19:59:42|3ecb666eda6d2ebb9...|\n",
      "|     2017-06-19 08:03:22|210d1686d116f7589...|\n",
      "|     2018-08-28 21:56:30|dbb786f88b6d4e52f...|\n",
      "|     2017-11-03 03:03:15|79aa91e33030a170c...|\n",
      "|     2018-01-12 21:59:20|3fa59277573f0fe06...|\n",
      "|     2018-02-10 09:25:05|44a10a741d51010c1...|\n",
      "|     2017-08-06 21:38:10|caa157284ffa71411...|\n",
      "|     2018-07-09 14:31:00|fb0a15c6b3645581d...|\n",
      "|     2017-09-05 09:29:17|a1fa82769a203e30b...|\n",
      "|     2018-03-03 09:57:04|e639eb028398f98b7...|\n",
      "|     2018-08-07 14:10:05|69b4682d3ab5ef1f1...|\n",
      "|     2018-04-11 10:45:45|efc21a26212414ea2...|\n",
      "|     2018-01-08 19:38:58|8b21fb2df5ea38807...|\n",
      "|     2017-10-26 10:13:29|b32fad437a7904a2d...|\n",
      "+------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Null Checking\")\n",
    "null_counts = query_result_order.select([col(c).alias(c) for c in query_result_order.columns]).na.drop().count()\n",
    "print(\"Number of null values in each column:\")\n",
    "query_result_order.select([col(c).alias(c) for c in query_result_order.columns]).na.drop().show()\n",
    "print(f\"Total null values: {query_result_order.count() - null_counts}\")\n",
    "\n",
    "# Duplicate data checking\n",
    "print(\"Duplicate Data Checking\")\n",
    "duplicate_count = query_result_order.count() - query_result_order.dropDuplicates().count()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Drop duplicate data\n",
    "query_result_order = query_result_order.dropDuplicates()\n",
    "print(\"After Removing Duplicate Data\")\n",
    "query_result_order.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes:\n",
      "root\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      "\n",
      "Dtypes After Conversion:\n",
      "root\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import col,date_format,hour\n",
    "\n",
    "print(\"Dtypes:\")\n",
    "query_result_order.printSchema()\n",
    "\n",
    "# Convert the 'order_purchase_timestamp' column to a timestamp type\n",
    "query_result_order = query_result_order.withColumn(\"order_purchase_timestamp\", col(\"order_purchase_timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Parse day name\n",
    "query_result_order = query_result_order.withColumn(\"Day\", date_format(col(\"order_purchase_timestamp\"), \"EEEE\"))\n",
    "\n",
    "# Parse hour\n",
    "query_result_order = query_result_order.withColumn(\"Hour\", hour(col(\"order_purchase_timestamp\")))\n",
    "\n",
    "# Check data types after conversion\n",
    "print(\"Dtypes After Conversion:\")\n",
    "query_result_order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "|Hour|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|\n",
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "|  23|   697|    676|      600|     532|   496|     422|   591|\n",
      "|  22|   962|    939|      855|     834|   675|     534|   857|\n",
      "|  21|  1084|    996|      927|     826|   700|     645|   862|\n",
      "|  20|   998|    946|      881|     816|   714|     709|   944|\n",
      "|  19|   921|    894|      822|     798|   757|     729|   881|\n",
      "|  18|   891|    852|      818|     759|   697|     693|   875|\n",
      "|  17|   954|    942|      943|     881|   790|     673|   776|\n",
      "|  16|  1060|   1059|     1007|    1042|   943|     675|   688|\n",
      "|  15|  1047|   1005|      952|     901|   947|     699|   697|\n",
      "|  14|  1063|   1095|     1027|     951|   938|     649|   660|\n",
      "|  13|  1000|   1011|      990|     953|   967|     685|   701|\n",
      "|  12|   944|    874|      897|     939|   827|     666|   653|\n",
      "|  11|  1052|   1023|     1030|     928|   960|     686|   705|\n",
      "|  10|   999|    940|     1007|     956|   928|     635|   513|\n",
      "|   9|   752|    845|      798|     736|   754|     421|   341|\n",
      "|   8|   471|    510|      497|     495|   486|     245|   202|\n",
      "|   7|   157|    216|      208|     211|   203|     102|   102|\n",
      "|   6|    64|     69|       90|      82|    91|      51|    30|\n",
      "|   5|    22|     24|       24|      28|    35|      23|    26|\n",
      "|   4|    21|     28|       33|      30|    39|      25|    27|\n",
      "+----+------+-------+---------+--------+------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "custom_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Use the 'when' function to create a custom sorting column\n",
    "day_hour_group = query_result_order.groupBy('Hour').pivot('Day').agg(count('order_id'))\n",
    "day_hour_group = day_hour_group.select(\n",
    "    ['Hour'] + [F.col(day).alias(day) for day in custom_order]\n",
    ")\n",
    "\n",
    "# Order the DataFrame by 'Hour'\n",
    "day_hour_group = day_hour_group.orderBy('Hour', ascending=False)\n",
    "\n",
    "# Show the result\n",
    "day_hour_group.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Payment Analysis: For each payment type (payment_type), calculate the total payment value (sum of payment_value) and the average number of payment installments (payment_installments), rename the columns to 'Total Payment Value' and 'Avg Installments,' and order the result by payment type in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+\n",
      "|Payment_type| Total Payment Value| Avg Installments|\n",
      "+------------+--------------------+-----------------+\n",
      "|      boleto|  2869361.2700000196|              1.0|\n",
      "| credit_card|1.2542084189999647E7|3.507155413763917|\n",
      "|  debit_card|  217989.79000000015|              1.0|\n",
      "| not_defined|                 0.0|              0.0|\n",
      "|     voucher|   379436.8700000001|              1.0|\n",
      "+------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the analysis\n",
    "payment_analysis = order_payment_df.groupBy(\"Payment_type\") \\\n",
    "    .agg(\n",
    "        sum(\"Payment_value\").alias(\"Total Payment Value\"),\n",
    "        avg(\"Payment_installments\").alias(\"Avg Installments\")\n",
    "    ) \\\n",
    "    .orderBy(\"Payment_type\")\n",
    "\n",
    "# Show the result\n",
    "payment_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Growth analysis: Determine the month-over-month sales growth percentage, using a window function to compare the current month's total sales with the previous month's total sales for each seller (seller_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "|           Seller_id|YearMonth|monthly_sales|prev_month_sales|sales_growth_percentage|\n",
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "|0015a82c2db000af6...|  2017-09|        895.0|            null|                   null|\n",
      "|0015a82c2db000af6...|  2017-10|       1790.0|           895.0|                  100.0|\n",
      "|001cca7ae9ae17fb1...|  2017-02|       1098.9|            null|                   null|\n",
      "|001cca7ae9ae17fb1...|  2017-03|       1676.7|          1098.9|     52.579844746649954|\n",
      "|001cca7ae9ae17fb1...|  2017-04|       1708.2|          1676.7|     1.8786903391977854|\n",
      "|001cca7ae9ae17fb1...|  2017-05|      2639.99|          1708.2|     54.548066133783976|\n",
      "|001cca7ae9ae17fb1...|  2017-06|      2213.49|         2639.99|     -16.15536428462503|\n",
      "|001cca7ae9ae17fb1...|  2017-07|      2483.95|         2213.49|       12.2187117236009|\n",
      "|001cca7ae9ae17fb1...|  2017-08|       1244.0|         2483.95|    -49.918475635424656|\n",
      "|001cca7ae9ae17fb1...|  2017-09|       1714.0|          1244.0|      37.78135048231511|\n",
      "|001cca7ae9ae17fb1...|  2017-10|       2626.0|          1714.0|      53.20886814469078|\n",
      "|001cca7ae9ae17fb1...|  2017-11|       2540.0|          2626.0|    -3.2749428789032753|\n",
      "|001cca7ae9ae17fb1...|  2017-12|       1330.0|          2540.0|     -47.63779527559055|\n",
      "|001cca7ae9ae17fb1...|  2018-01|        914.0|          1330.0|    -31.278195488721806|\n",
      "|001cca7ae9ae17fb1...|  2018-02|        331.0|           914.0|      -63.7855579868709|\n",
      "|001cca7ae9ae17fb1...|  2018-03|       1056.9|           331.0|     219.30514332751133|\n",
      "|001cca7ae9ae17fb1...|  2018-04|        539.0|          1056.9|    -49.001798888327436|\n",
      "|001cca7ae9ae17fb1...|  2018-06|        241.0|           539.0|    -55.287569573283854|\n",
      "|001cca7ae9ae17fb1...|  2018-07|        129.9|           241.0|    -46.099587594819766|\n",
      "|002100f778ceb8431...|  2017-09|         47.6|            null|                   null|\n",
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the orders and order_items datasets\n",
    "combined_df = orders_df.join(order_item_df, \"Order_id\")\n",
    "\n",
    "# Calculate monthly sales for each seller\n",
    "monthly_sales_window = Window.partitionBy(\"Seller_id\").orderBy(\"YearMonth\")\n",
    "combined_df = combined_df.withColumn(\"YearMonth\", combined_df[\"Order_purchase_timestamp\"].substr(1, 7))\n",
    "monthly_sales_df = combined_df.groupBy(\"Seller_id\", \"YearMonth\").agg(sum(\"Price\").alias(\"monthly_sales\"))\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"monthly_sales\", monthly_sales_df[\"monthly_sales\"].cast(\"float\"))\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"prev_month_sales\", lag(\"monthly_sales\").over(monthly_sales_window))\n",
    "\n",
    "# Calculate month-over-month sales growth percentage\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"sales_growth_percentage\",\n",
    "                                               ((monthly_sales_df[\"monthly_sales\"] - monthly_sales_df[\"prev_month_sales\"]) /\n",
    "                                                monthly_sales_df[\"prev_month_sales\"]) * 100)\n",
    "\n",
    "# Show the result\n",
    "monthly_sales_df.select(\"Seller_id\", \"YearMonth\", \"monthly_sales\", \"prev_month_sales\", \"sales_growth_percentage\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Total number of orders placed by customers in each state (customer_state), rename the column to 'Order Count,' and order the result in ascending order of order count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|customer_state|Order Count|\n",
      "+--------------+-----------+\n",
      "|            RR|         41|\n",
      "|            AP|         67|\n",
      "|            AC|         80|\n",
      "|            AM|        145|\n",
      "|            RO|        243|\n",
      "|            TO|        274|\n",
      "|            SE|        335|\n",
      "|            AL|        397|\n",
      "|            RN|        474|\n",
      "|            PI|        476|\n",
      "|            PB|        517|\n",
      "|            MS|        701|\n",
      "|            MA|        716|\n",
      "|            MT|        886|\n",
      "|            PA|        946|\n",
      "|            CE|       1278|\n",
      "|            PE|       1593|\n",
      "|            GO|       1957|\n",
      "|            ES|       1995|\n",
      "|            DF|       2080|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join customers and orders datasets to identify unique customers in the orders dataset\n",
    "unique_customers_df = customer_df.join(orders_df, \"Customer_id\")\n",
    "\n",
    "# Group by customer_state and count the number of orders in each state\n",
    "order_count_by_state = unique_customers_df.groupBy(\"customer_state\").agg(count(\"Order_id\").alias(\"Order Count\"))\n",
    "\n",
    "# Order the result in ascending order of order count\n",
    "order_count_by_state = order_count_by_state.orderBy(\"Order Count\")\n",
    "\n",
    "# Show the result\n",
    "order_count_by_state.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Calculate the order count per customer, computes the average order value per customer, create a customer and order analysis DataFrame, and display the result. Also print the total number of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|         Customer_id|Order_Count|Average_Order_Value|\n",
      "+--------------------+-----------+-------------------+\n",
      "|1617b1357756262bf...|          1|           13664.08|\n",
      "|ec5b2ba62e5743423...|          1|            7274.88|\n",
      "|c6e2731c5b391845f...|          1|            6929.31|\n",
      "|f48d464a0baaea338...|          1|            6922.21|\n",
      "|3fd6777bbce08a352...|          1|            6726.66|\n",
      "|05455dfa7cd02f13d...|          1|            6081.54|\n",
      "|df55c14d1476a9a34...|          1|            4950.34|\n",
      "|24bbf5fd2f2e1b359...|          1|            4764.34|\n",
      "|3d979689f636322c6...|          1|            4681.78|\n",
      "|1afc82cd60e303ef0...|          1|            4513.32|\n",
      "|cc803a2c412833101...|          1|             4445.5|\n",
      "|35a413c7ca3c69756...|          1|            4175.26|\n",
      "|e9b0d0eb3015ef1c9...|          1|            4163.51|\n",
      "|3be2c536886b2ea46...|          1|            4042.74|\n",
      "|c6695e3b1e48680db...|          1|            4016.91|\n",
      "|31e83c01fce824d0f...|          1|            3979.55|\n",
      "|addc91fdf9c2b3045...|          1|             3826.8|\n",
      "|19b32919fa1198aef...|          1|            3792.59|\n",
      "|66657bf1753d82d0a...|          1|            3736.22|\n",
      "|7d03bf20fa96e8046...|          1|            3666.42|\n",
      "+--------------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total NUmber of Customer =  96460\n"
     ]
    }
   ],
   "source": [
    "# Join the Orders DataFrame and Order Payments DataFrame based on \"Order_id\"\n",
    "order_payment_joined_df = orders_df.join(order_payment_df , \"Order_id\")\n",
    "\n",
    "# Calculate order count per customer\n",
    "order_count = orders_df.groupBy(\"Customer_id\").agg(count(\"Order_id\").alias(\"Order_Count\"))\n",
    "\n",
    "\n",
    "# Calculate average order value per customer\n",
    "average_order_value = order_payment_joined_df.groupBy(\"Customer_id\").agg(avg(\"Payment_value\").alias(\"Average_Order_Value\"))\n",
    "\n",
    "\n",
    "# Create a customer and order analysis DataFrame\n",
    "customer_behaviour_df = order_count.join(average_order_value, \"Customer_id\", \"inner\").orderBy(desc(\"Average_Order_Value\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "customer_behaviour_df.show()\n",
    "print(\"Total NUmber of Customer = \", customer_behaviour_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Create a UDF to categorize products into different size categories based on their dimensions (length, width, and height) and weight? Then, calculate the average order value for each product size category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|Product_Size_Category|Average_Order_Value|\n",
      "+---------------------+-------------------+\n",
      "|              Unknown|             138.72|\n",
      "|               Medium|             101.92|\n",
      "|                Small|              82.06|\n",
      "|                Large|             175.04|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to categorize products into size categories based on dimensions and weight\n",
    "def categorize_product_size(length, width, height, weight):\n",
    "    if length is not None and width is not None and height is not None and weight is not None:\n",
    "        if length <= 20 and width <= 20 and height <= 20 and weight <= 500:\n",
    "            return \"Small\"\n",
    "        elif length <= 40 and width <= 40 and height <= 40 and weight <= 2000:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Register the UDF\n",
    "categorize_udf = udf(categorize_product_size, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column \"Product_Size_Category\"\n",
    "product_df = product_df.withColumn(\"Product_Size_Category\", categorize_udf(\n",
    "    product_df[\"Product_length_cm\"],\n",
    "    product_df[\"Product_width_cm\"],\n",
    "    product_df[\"Product_height_cm\"],\n",
    "    product_df[\"Product_weight_g\"]\n",
    "))\n",
    "\n",
    "# Join the necessary datasets\n",
    "joined_df = order_item_df.join(product_df, \"Product_id\")\n",
    "\n",
    "# Calculate the average order value for each product size category\n",
    "average_order_value_by_size = joined_df.groupBy(\"Product_Size_Category\").agg(\n",
    "    round(avg(\"Price\"),2).alias(\"Average_Order_Value\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "average_order_value_by_size.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

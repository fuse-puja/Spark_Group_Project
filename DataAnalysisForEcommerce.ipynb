{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining path to the dataset\n",
    "customer_data_path = \"./Data/olist_customers_dataset.csv\"  # Replace with the actual path\n",
    "order_item_path = \"./Data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"./Data/olist_order_payments_dataset.csv\"\n",
    "product_category_translation_path= \"./Data/product_category_name_translation.csv\"\n",
    "product_path = './Data/olist_products_dataset.csv'\n",
    "seller_path = './Data/olist_sellers_dataset.csv'\n",
    "geolocation_path = './Data/olist_geolocation_dataset.csv'\n",
    "orders_path = './Data/olist_orders_dataset.csv'\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "customer_df = spark.read.csv(customer_data_path, header=True, inferSchema=True)\n",
    "order_item_df = spark.read.csv(order_item_path, header=True, inferSchema=True)\n",
    "order_payment_df = spark.read.csv(order_payment_path, header=True, inferSchema=True)\n",
    "product_category_translation_df = spark.read.csv(product_category_translation_path, header=True, inferSchema=True)\n",
    "seller_df_uncleaned = spark.read.csv(seller_path, header=True, inferSchema=True)\n",
    "product_df_uncleaned = spark.read.csv(product_path, header=True, inferSchema=True)\n",
    "geoloacation_df_uncleaned = spark.read.csv(geolocation_path, header=True, inferSchema= True)\n",
    "orders_df_uncleaned = spark.read.csv(orders_path, header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim,regexp_replace, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "seller_df_uncleaned.select([trim(col(c)).alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "# Remove whitespace characters between words in all columns\n",
    "seller_df = seller_df_uncleaned.select([regexp_replace(col(c), r'\\s+', ' ').alias(c) for c in seller_df_uncleaned.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       são paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       são paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       são paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing whitespace from all columns\n",
    "geoloacation_df_uncleaned.select([trim(col(c)).alias(c) for c in geoloacation_df_uncleaned.columns])\n",
    "\n",
    "geoloacation_df_uncleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268| -46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469| -46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681| -46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493| -46.64160722329613|       sao paulo|               SP|\n",
      "|                       1012|-23.547762303364266| -46.63536053788448|       sao paulo|               SP|\n",
      "|                       1047|-23.546273112412678| -46.64122516971552|       sao paulo|               SP|\n",
      "|                       1013|-23.546923208436723|  -46.6342636964915|       sao paulo|               SP|\n",
      "|                       1029|-23.543769055769133| -46.63427784085132|       sao paulo|               SP|\n",
      "|                       1011|-23.547639550320632| -46.63603162315495|       sao paulo|               SP|\n",
      "|                       1013|-23.547325128224376| -46.63418378613892|       sao paulo|               SP|\n",
      "|                       1032| -23.53841810407414|-46.634778375266734|       sao paulo|               SP|\n",
      "|                       1014|-23.546435343326205| -46.63383023397196|       sao paulo|               SP|\n",
      "|                       1012|-23.548945985189434| -46.63467113292871|       sao paulo|               SP|\n",
      "|                       1037|-23.545187340816042| -46.63785524104107|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535| -46.64482029837157|       sao paulo|               SP|\n",
      "|                       1039|-23.541883009983316| -46.63991946670314|       sao paulo|               SP|\n",
      "|                       1024|-23.541389521053937|-46.629899087812184|       sao paulo|               SP|\n",
      "|                       1009| -23.54693540437998| -46.63658792659698|       sao paulo|               SP|\n",
      "|                       1046|-23.545884279214015|-46.643163191240035|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace \"são paulo\" with \"sao paulo\" in the geolocation dataframe\n",
    "geolocation_df = geoloacation_df_uncleaned.replace(\"são paulo\", \"sao paulo\")\n",
    "\n",
    "# Show the DataFrame with the replaced values\n",
    "geolocation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows in uncleaned dataset =  99441\n",
      "No of rows of cleaned datset =  96461\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the 'orders_df_uncleaned' DataFrame\n",
    "print(\"No of rows in uncleaned dataset = \", orders_df_uncleaned.count())\n",
    "\n",
    "# Drop rows with null values in the 'orders_df_uncleaned' DataFrame\n",
    "orders_df = orders_df_uncleaned.dropna()\n",
    "\n",
    "# Print the number of rows in the 'orders_df' DataFrame after dropping null values\n",
    "print(\"No of rows of cleaned datset = \", orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing column on product dataset with content from product category translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|          product_id|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_category_name|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "|1e9e8ef04dbcff454...|                 40|                       287|                 1|             225|               16|               10|              14|            perfumery|\n",
      "|3aa071139cb16b67c...|                 44|                       276|                 1|            1000|               30|               18|              20|                  art|\n",
      "|96bd76ec8810374ed...|                 46|                       250|                 1|             154|               18|                9|              15|       sports_leisure|\n",
      "|cef67bcfe19066a93...|                 27|                       261|                 1|             371|               26|                4|              26|                 baby|\n",
      "|9dc1a7de274444849...|                 37|                       402|                 4|             625|               20|               17|              13|           housewares|\n",
      "|41d3672d4792049fa...|                 60|                       745|                 1|             200|               38|                5|              11|  musical_instruments|\n",
      "|732bd381ad09e530f...|                 56|                      1272|                 4|           18350|               70|               24|              44|           cool_stuff|\n",
      "|2548af3e6e77a690c...|                 56|                       184|                 2|             900|               40|                8|              40|      furniture_decor|\n",
      "|37cc742be07708b53...|                 57|                       163|                 1|             400|               27|               13|              17|      home_appliances|\n",
      "|8c92109888e8cdf9d...|                 36|                      1156|                 1|             600|               17|               10|              12|                 toys|\n",
      "|14aa47b7fe5c25522...|                 54|                       630|                 1|            1100|               16|               10|              16|       bed_bath_table|\n",
      "|03b63c5fc16691530...|                 49|                       728|                 4|            7150|               50|               19|              45|                 baby|\n",
      "|cf55509ea8edaaac1...|                 43|                      1827|                 3|             250|               17|                7|              17|  musical_instruments|\n",
      "|7bb6f29c2be577161...|                 51|                      2083|                 2|             600|               68|               11|              13|      furniture_decor|\n",
      "|eb31436580a610f20...|                 59|                      1602|                 4|             200|               17|                7|              17| construction_tool...|\n",
      "|3bb7f144022e67327...|                 22|                      3021|                 1|             800|               16|                2|              11|       sports_leisure|\n",
      "|6a2fb4dd53d2cdb88...|                 39|                       346|                 2|             400|               27|                5|              20|            perfumery|\n",
      "|a1b71017a84f92fd8...|                 59|                       636|                 1|             900|               40|               15|              20| computers_accesso...|\n",
      "|a0736b92e52f6cead...|                 56|                       296|                 2|            1700|              100|                7|              15|      furniture_decor|\n",
      "|f53103a77d9cf245e...|                 52|                       206|                 1|             500|               16|               10|              16|       bed_bath_table|\n",
      "+--------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a left join between the 'product_df_uncleaned' DataFrame and 'product_category_translation_df'\n",
    "# based on the 'Product_category_name' column. This operation combines the two DataFrames .\n",
    "product_joined_df= product_df_uncleaned.join(product_category_translation_df, \"Product_category_name\", \"left\")\n",
    "\n",
    "# Drop \"product_category_name\" will be removed from the DataFrame.\n",
    "product_df = product_joined_df.drop(\"product_category_name\")\n",
    "\n",
    "# Rename the \"product_category_name_english\" column to \"product_category_name\"\n",
    "product_df = product_df.withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# Show the 'product_df' DataFrame with the dropped and renamed columns.\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set payment_installment to 0 where payment_type is \"not_defined\"\n",
    "order_payment_df = order_payment_df.withColumn(\"Payment_installments\",\n",
    "                                   when(col(\"Payment_type\") == \"not_defined\", 0)\n",
    "                                   .otherwise(col(\"Payment_installments\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformation on the Dataframes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Dataframes:\n",
    "    -customer_df \n",
    "    -order_item_df \n",
    "    -order_payment_df \n",
    "    -product_category_translation_df \n",
    "    -seller_df\n",
    "    -product_df\n",
    "    -geoloacation_df\n",
    "    -orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, lag, count, desc, udf,round\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Payment Analysis: For each payment type (payment_type), calculate the total payment value (sum of payment_value) and the average number of payment installments (payment_installments), rename the columns to 'Total Payment Value' and 'Avg Installments,' and order the result by payment type in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+\n",
      "|Payment_type| Total Payment Value| Avg Installments|\n",
      "+------------+--------------------+-----------------+\n",
      "|      boleto|  2869361.2700000196|              1.0|\n",
      "| credit_card|1.2542084189999647E7|3.507155413763917|\n",
      "|  debit_card|  217989.79000000015|              1.0|\n",
      "| not_defined|                 0.0|              0.0|\n",
      "|     voucher|   379436.8700000001|              1.0|\n",
      "+------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the analysis\n",
    "payment_analysis = order_payment_df.groupBy(\"Payment_type\") \\\n",
    "    .agg(\n",
    "        sum(\"Payment_value\").alias(\"Total Payment Value\"),\n",
    "        avg(\"Payment_installments\").alias(\"Avg Installments\")\n",
    "    ) \\\n",
    "    .orderBy(\"Payment_type\")\n",
    "\n",
    "# Show the result\n",
    "payment_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Growth analysis: Determine the month-over-month sales growth percentage, using a window function to compare the current month's total sales with the previous month's total sales for each seller (seller_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "|           Seller_id|YearMonth|monthly_sales|prev_month_sales|sales_growth_percentage|\n",
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "|0015a82c2db000af6...|  2017-09|        895.0|            null|                   null|\n",
      "|0015a82c2db000af6...|  2017-10|       1790.0|           895.0|                  100.0|\n",
      "|001cca7ae9ae17fb1...|  2017-02|       1098.9|            null|                   null|\n",
      "|001cca7ae9ae17fb1...|  2017-03|       1676.7|          1098.9|     52.579844746649954|\n",
      "|001cca7ae9ae17fb1...|  2017-04|       1708.2|          1676.7|     1.8786903391977854|\n",
      "|001cca7ae9ae17fb1...|  2017-05|      2639.99|          1708.2|     54.548066133783976|\n",
      "|001cca7ae9ae17fb1...|  2017-06|      2213.49|         2639.99|     -16.15536428462503|\n",
      "|001cca7ae9ae17fb1...|  2017-07|      2483.95|         2213.49|       12.2187117236009|\n",
      "|001cca7ae9ae17fb1...|  2017-08|       1244.0|         2483.95|    -49.918475635424656|\n",
      "|001cca7ae9ae17fb1...|  2017-09|       1714.0|          1244.0|      37.78135048231511|\n",
      "|001cca7ae9ae17fb1...|  2017-10|       2626.0|          1714.0|      53.20886814469078|\n",
      "|001cca7ae9ae17fb1...|  2017-11|       2540.0|          2626.0|    -3.2749428789032753|\n",
      "|001cca7ae9ae17fb1...|  2017-12|       1330.0|          2540.0|     -47.63779527559055|\n",
      "|001cca7ae9ae17fb1...|  2018-01|        914.0|          1330.0|    -31.278195488721806|\n",
      "|001cca7ae9ae17fb1...|  2018-02|        331.0|           914.0|      -63.7855579868709|\n",
      "|001cca7ae9ae17fb1...|  2018-03|       1056.9|           331.0|     219.30514332751133|\n",
      "|001cca7ae9ae17fb1...|  2018-04|        539.0|          1056.9|    -49.001798888327436|\n",
      "|001cca7ae9ae17fb1...|  2018-06|        241.0|           539.0|    -55.287569573283854|\n",
      "|001cca7ae9ae17fb1...|  2018-07|        129.9|           241.0|    -46.099587594819766|\n",
      "|002100f778ceb8431...|  2017-09|         47.6|            null|                   null|\n",
      "+--------------------+---------+-------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the orders and order_items datasets\n",
    "combined_df = orders_df.join(order_item_df, \"Order_id\")\n",
    "\n",
    "# Calculate monthly sales for each seller\n",
    "monthly_sales_window = Window.partitionBy(\"Seller_id\").orderBy(\"YearMonth\")\n",
    "combined_df = combined_df.withColumn(\"YearMonth\", combined_df[\"Order_purchase_timestamp\"].substr(1, 7))\n",
    "monthly_sales_df = combined_df.groupBy(\"Seller_id\", \"YearMonth\").agg(sum(\"Price\").alias(\"monthly_sales\"))\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"monthly_sales\", monthly_sales_df[\"monthly_sales\"].cast(\"float\"))\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"prev_month_sales\", lag(\"monthly_sales\").over(monthly_sales_window))\n",
    "\n",
    "# Calculate month-over-month sales growth percentage\n",
    "monthly_sales_df = monthly_sales_df.withColumn(\"sales_growth_percentage\",\n",
    "                                               ((monthly_sales_df[\"monthly_sales\"] - monthly_sales_df[\"prev_month_sales\"]) /\n",
    "                                                monthly_sales_df[\"prev_month_sales\"]) * 100)\n",
    "\n",
    "# Show the result\n",
    "monthly_sales_df.select(\"Seller_id\", \"YearMonth\", \"monthly_sales\", \"prev_month_sales\", \"sales_growth_percentage\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Total number of orders placed by customers in each state (customer_state), rename the column to 'Order Count,' and order the result in ascending order of order count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|customer_state|Order Count|\n",
      "+--------------+-----------+\n",
      "|            RR|         41|\n",
      "|            AP|         67|\n",
      "|            AC|         80|\n",
      "|            AM|        145|\n",
      "|            RO|        243|\n",
      "|            TO|        274|\n",
      "|            SE|        335|\n",
      "|            AL|        397|\n",
      "|            RN|        474|\n",
      "|            PI|        476|\n",
      "|            PB|        517|\n",
      "|            MS|        701|\n",
      "|            MA|        716|\n",
      "|            MT|        886|\n",
      "|            PA|        946|\n",
      "|            CE|       1278|\n",
      "|            PE|       1593|\n",
      "|            GO|       1957|\n",
      "|            ES|       1995|\n",
      "|            DF|       2080|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join customers and orders datasets to identify unique customers in the orders dataset\n",
    "unique_customers_df = customer_df.join(orders_df, \"Customer_id\")\n",
    "\n",
    "# Group by customer_state and count the number of orders in each state\n",
    "order_count_by_state = unique_customers_df.groupBy(\"customer_state\").agg(count(\"Order_id\").alias(\"Order Count\"))\n",
    "\n",
    "# Order the result in ascending order of order count\n",
    "order_count_by_state = order_count_by_state.orderBy(\"Order Count\")\n",
    "\n",
    "# Show the result\n",
    "order_count_by_state.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+\n",
      "|         Customer_id|Order_Count|Average_Order_Value|\n",
      "+--------------------+-----------+-------------------+\n",
      "|1617b1357756262bf...|          1|           13664.08|\n",
      "|ec5b2ba62e5743423...|          1|            7274.88|\n",
      "|c6e2731c5b391845f...|          1|            6929.31|\n",
      "|f48d464a0baaea338...|          1|            6922.21|\n",
      "|3fd6777bbce08a352...|          1|            6726.66|\n",
      "|05455dfa7cd02f13d...|          1|            6081.54|\n",
      "|df55c14d1476a9a34...|          1|            4950.34|\n",
      "|24bbf5fd2f2e1b359...|          1|            4764.34|\n",
      "|3d979689f636322c6...|          1|            4681.78|\n",
      "|1afc82cd60e303ef0...|          1|            4513.32|\n",
      "|cc803a2c412833101...|          1|             4445.5|\n",
      "|35a413c7ca3c69756...|          1|            4175.26|\n",
      "|e9b0d0eb3015ef1c9...|          1|            4163.51|\n",
      "|3be2c536886b2ea46...|          1|            4042.74|\n",
      "|c6695e3b1e48680db...|          1|            4016.91|\n",
      "|31e83c01fce824d0f...|          1|            3979.55|\n",
      "|addc91fdf9c2b3045...|          1|             3826.8|\n",
      "|19b32919fa1198aef...|          1|            3792.59|\n",
      "|66657bf1753d82d0a...|          1|            3736.22|\n",
      "|7d03bf20fa96e8046...|          1|            3666.42|\n",
      "+--------------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total NUmber of Customer =  96460\n"
     ]
    }
   ],
   "source": [
    "# Join the Orders DataFrame and Order Payments DataFrame based on \"Order_id\"\n",
    "order_payment_joined_df = orders_df.join(order_payment_df , \"Order_id\")\n",
    "\n",
    "# Calculate order count per customer\n",
    "order_count = orders_df.groupBy(\"Customer_id\").agg(count(\"Order_id\").alias(\"Order_Count\"))\n",
    "\n",
    "\n",
    "# Calculate average order value per customer\n",
    "average_order_value = order_payment_joined_df.groupBy(\"Customer_id\").agg(avg(\"Payment_value\").alias(\"Average_Order_Value\"))\n",
    "\n",
    "\n",
    "# Create a customer and order analysis DataFrame\n",
    "customer_behaviour_df = order_count.join(average_order_value, \"Customer_id\", \"inner\").orderBy(desc(\"Average_Order_Value\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "customer_behaviour_df.show()\n",
    "print(\"Total NUmber of Customer = \", customer_behaviour_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Create a UDF to categorize products into different size categories based on their dimensions (length, width, and height) and weight? Then, calculate the average order value for each product size category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|Product_Size_Category|Average_Order_Value|\n",
      "+---------------------+-------------------+\n",
      "|              Unknown|             138.72|\n",
      "|               Medium|             101.92|\n",
      "|                Small|              82.06|\n",
      "|                Large|             175.04|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to categorize products into size categories based on dimensions and weight\n",
    "def categorize_product_size(length, width, height, weight):\n",
    "    if length is not None and width is not None and height is not None and weight is not None:\n",
    "        if length <= 20 and width <= 20 and height <= 20 and weight <= 500:\n",
    "            return \"Small\"\n",
    "        elif length <= 40 and width <= 40 and height <= 40 and weight <= 2000:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Register the UDF\n",
    "categorize_udf = udf(categorize_product_size, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column \"Product_Size_Category\"\n",
    "product_df = product_df.withColumn(\"Product_Size_Category\", categorize_udf(\n",
    "    product_df[\"Product_length_cm\"],\n",
    "    product_df[\"Product_width_cm\"],\n",
    "    product_df[\"Product_height_cm\"],\n",
    "    product_df[\"Product_weight_g\"]\n",
    "))\n",
    "\n",
    "# Join the necessary datasets\n",
    "joined_df = order_item_df.join(product_df, \"Product_id\")\n",
    "\n",
    "# Calculate the average order value for each product size category\n",
    "average_order_value_by_size = joined_df.groupBy(\"Product_Size_Category\").agg(\n",
    "    round(avg(\"Price\"),2).alias(\"Average_Order_Value\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "average_order_value_by_size.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
